---
title: "Topic modeling demo"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

### Load necessary libraries
```{r libraries}
library(tidyverse)
library(tidymodels)
library(tidylo)
library(stm)
library(purrr)
library(RColorBrewer)
library(ggwordcloud)
library(ggrepel)
library(patchwork)

#devtools::install_github("juliasilge/tidytext", force = TRUE) 
library(tidytext)

theme_set(theme_bw())
```

### Load data: publicly available text datasets for the Office dialogue across all seasons:
```{r data}
library(schrute)
df_office <- schrute::theoffice
speaking_characters <- df_office %>% 
  count(character, sort = TRUE) %>% 
  
  # select only characters with at least 500 lines across all seasons
  filter(n > 500) %>% 
  pull(character)
```        

### Unnest tokens: restructure as one-token-per-row (each word is a separate row in a tibble)
```{r unnest_tokens}
df_tokens <- df_office %>% 
  
  # filter for our speaking characters
  filter(character %in% speaking_characters) %>% 

  group_by(character) %>% 
  unnest_tokens(word, text) %>% 
  
  # remove stop words (uninteresting words like of, from, and, the, etc.)
  anti_join(get_stopwords(), by = join_by(word))

head(df_tokens)
```

### Create a sparse matrix: each row is a corpus (book) and each column is a word
```{r sparse_matrix}
sparse_mat <- df_tokens %>%
  
  # count number of times a word is used by each character
  dplyr::count(character, word) %>%
  
  # filter for words that appear at least 3 times
  filter(n > 3) %>%
  
  # create sparse matrix
  cast_sparse(character, word, n)

dim(sparse_mat)
sparse_mat[1:10, 1:10]
```

### Fit topic model using Structural Topic Models (http://www.structuraltopicmodel.com/)
```{r topic_model}
set.seed(0306)
topic_model <- stm(sparse_mat, 
                   K = 4, # K is the number of 'topics' (clusters)
                   verbose = FALSE)

summary(topic_model)
```

**lift** = frequency divided by frequency in other topics
**FREX** weights words by frequency and exclusivity to the topic

## Ta-da!!

But.... how can we determine the optimal number of topics for our dataset?

```{r many_models}
df_many_models <- tibble(K = seq(3, 10, by = 1)) %>% 
  mutate(topic_model = map (
    
    .x = K,
    .f = ~ stm(sparse_mat, 
               K = .x, 
               verbose = FALSE)
    
  ))

head(df_many_models)
```

Now that we’ve fit all these topic models with different numbers of topics, we can explore how many topics are appropriate/good/“best”

```{r diagnostics, fig.height = 3}
df_diagnostics <- df_many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, sparse_mat),
         residual = map(topic_model, checkResiduals, sparse_mat),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

head(df_diagnostics)

df_diagnostics %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean)) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(linewidth = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics")
```

Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and it’s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. It’s a tradeoff. Read more about semantic coherence in the original paper about it (https://dl.acm.org/citation.cfm?id=2145462.

```{r semantic_coherence, fig.height = 3}
df_diagnostics %>%
  select(K, exclusivity, semantic_coherence) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  filter(K %in% c(3, 5, 7)) %>% 
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence")
```

In choosing our final topic model, we should go with a K value that is statistically sound, but also contextually (in this case, biologically) meaningful and informative

### Finalize model
```{r final_model, fig.height = 6, fig.width = 8}
topic_model <- stm(sparse_mat, 
                   K = 6, # K is the number of 'topics' (clusters)
                   verbose = FALSE)

p_beta <- tidy(topic_model, matrix = "frex") %>%
  left_join(tidy(topic_model)) %>% 
  group_by(topic) %>%
  slice_head(n = 20) %>%
  mutate(topic = paste0("topic ", topic)) %>% 

  ggplot(aes(x = beta, y = reorder(term, beta), fill = topic)) +
  geom_col() +
  labs(y = "Word") +
  facet_wrap(vars(topic), scales = "free", nrow = 1)

# Topic probabilities
group_gamma <- tidy(
  topic_model, 
  matrix = "gamma",
  document_names = rownames(sparse_mat)) %>% 
  mutate(chapter = factor(document)) %>% 
  dplyr::select(chapter, topic, gamma) %>% 
  mutate(topic = factor(topic)
  )

p_gamma <- group_gamma %>%
  filter(gamma > 0.01 ) %>% 
  ggplot(aes(x = topic, y = gamma, color = topic)) +
  geom_point(aes(alpha = gamma)) +
  geom_text_repel(aes(alpha = gamma, label = chapter), size = 3, 
                  max.overlaps = 50) +

  labs(y = expression(gamma), x = "") +
  ggtitle("Corpus (character) strength of association with each topic") +
  theme(legend.position = "none",
        axis.text.x = element_blank())

(p_gamma / p_beta)
```

